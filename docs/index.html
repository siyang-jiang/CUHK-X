<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CUHK-X: Multimodal Dataset for Human Action Recognition, Understanding and Reasoning</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    /* ========= General Layout ========= */
    body {
      font-family: 'Inter', sans-serif;
      color: #222;
      background: #fafafa;
      margin: 0;
      padding: 0;
      line-height: 1.6;
    }

    html {
      scroll-behavior: smooth;
    }

    main, header, section, footer {
      max-width: 950px;
      margin: 0 auto;
      padding: 40px 20px;
    }

    /* ========= Navbar ========= */
    .navbar {
      position: sticky;
      top: 0;
      width: 100%;
      background: white;
      border-bottom: 1px solid #eee;
      z-index: 999;
    }

    .nav-container {
      max-width: 950px;
      margin: 0 auto;
      display: flex;
      justify-content: center;
      padding: 12px 20px;
      gap: 24px;
    }

    .nav-container a {
      text-decoration: none;
      color: #333;
      font-weight: 500;
      transition: color 0.2s ease;
    }

    .nav-container a:hover {
      color: #007acc;
    }

    /* ========= Hero Banner ========= */
    .hero {
      text-align: center;
      padding: 100px 20px 60px;
      background: linear-gradient(180deg, #fdfdfd 0%, #fafafa 100%);
    }

    .hero h1 {
      font-size: 3em;
      font-weight: 700;
      margin-bottom: 20px;
    }

    .hero .authors {
      font-size: 1.2em;
      color: #555;
      margin-bottom: 10px;
    }

    .hero .affiliation {
      font-size: 1.1em;
      color: #666;
      margin-bottom: 30px;
      font-style: italic;
    }

    .hero .links a {
      display: inline-block;
      margin: 0 12px;
      padding: 10px 18px;
      border: 1px solid #333;
      border-radius: 10px;
      text-decoration: none;
      color: #333;
      font-weight: 500;
      transition: all 0.2s ease;
    }

    .hero .links a:hover {
      background: #333;
      color: white;
    }

    /* ========= Section Titles ========= */
    section h2 {
      text-align: center;
      font-size: 2em;
      margin-bottom: 20px;
      font-weight: 600;
    }

    /* ========= Media (images, videos) ========= */
    .media {
      display: flex;
      justify-content: center;
      margin: 40px 0;
    }

    .preview {
      max-width: 90%;
      border: 1px solid #ddd;
      border-radius: 12px;
      box-shadow: 0 4px 16px rgba(0,0,0,0.08);
      transition: transform 0.2s ease;
    }

    .preview:hover {
      transform: scale(1.02);
    }

    .video-container {
      display: flex;
      justify-content: center;
      align-items: center;
      gap: 20px;
      flex-wrap: wrap;
      margin-top: 30px;
    }

    .video-container .preview {
      width: 440px;   /* equal width for both videos */
      height: auto;
      border: 1px solid #ddd;
      border-radius: 12px;
      box-shadow: 0 4px 16px rgba(0,0,0,0.08);
      max-width: 100%; /* ensure responsiveness */
    }

    /* Video specific styling */
    video {
      display: block;
      width: 100%;
      height: auto;
    }

    /* Responsive video container */
    @media (max-width: 768px) {
      .video-container {
        flex-direction: column;
      }
      
      .video-container .preview {
        width: 100%;
        max-width: 400px;
      }
    }

    .video-block {
      text-align: center; /* centers caption and video */
    }

    .caption {
      font-weight: bold;
      text-align: center;
      margin-bottom: 8px;
      font-size: 1rem;
      color: #333;
    }

    .image-block {
      text-align: center;
      margin-top: 30px;
    }

    .image-block .caption {
      font-weight: bold;
      margin-bottom: 8px;
      font-size: 1rem;
      color: #333;
    }

    /* ========= Scrollable Image ========= */
    .scroll-container {
      max-width: 100%;
      height: 600px;
      overflow-y: auto;
      overflow-x: hidden;
      border: 1px solid #ddd;
      border-radius: 8px;
      margin: 30px auto;
    }

    .scroll-container img {
      display: block;
      width: 100%;
      height: auto;
    }

    .scroll-container img.small-image {
      width: 60%;         /* try 20% or even 10% */
      max-width: none;    /* disables any previous max-width rule */
      height: auto;
      display: block;
      margin: 0 auto;
    }

    /* ========= Code block (Citation) ========= */
    pre {
      background: #f4f4f4;
      padding: 16px;
      border-radius: 8px;
      overflow-x: auto;
      font-size: 0.9em;
      max-width: 800px;
      margin: 0 auto;
    }

    /* ========= Footer / Acknowledgments ========= */
    footer {
      text-align: center;
      padding: 60px 20px;
      background: #f9f9f9;
      border-top: 1px solid #eee;
      margin-top: 80px;
    }

    footer h2 {
      font-size: 1.8em;
      margin-bottom: 20px;
    }

    footer p {
      font-size: 1em;
      color: #444;
      max-width: 800px;
      margin: 0 auto;
      line-height: 1.6;
    }

    footer a {
      color: #0077cc;
      text-decoration: none;
      font-weight: 500;
    }

    footer a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <nav class="navbar">
    <div class="nav-container">
      <a href="#abstract">Abstract</a>
      <a href="#dataset">Dataset</a>
      <a href="#benchmarks">Benchmarks</a>
      <a href="#results">Results</a>
      <a href="#data-visualization">Visualization</a>
      <a href="#citation">Citation</a>
    </div>
  </nav>

  <header class="hero">
    <h1>CUHK-X</h1>
    <h2>A Large-Scale Multimodal Dataset and Benchmark for Human Action Recognition, Understanding and Reasoning</h2>
    <p class="authors">
    Siyang Jiang, Mu Yuan, Xiang Ji, Bufang Yang, Zeyu Liu, Lilin Xu, Yang Li, Yuting He, Liran Dong, 
    Zhenyu Yan, Xiaofan Jiang, Wei Gao, Hongkai Chen, Guoliang Xing.
    </p>
    <p class="affiliation">The Chinese University of Hong Kong</p>
    <div class="links">
      <a href="../paper/cuhkx.pdf">[Paper]</a>
      <a href="#">[Dataset]</a>
      <a href="#">[Code]</a>
      <a href="#data-visualization">[Demo]</a>
    </div>
  </header>

  <section id="abstract">
    <h2>Abstract</h2>
    <p>
      CUHK-X is a comprehensive multimodal dataset containing <strong>36,414 samples</strong> across <strong>seven modalities</strong> 
      designed for human activity recognition, understanding, and reasoning. Unlike existing datasets that focus primarily on recognition tasks, 
      CUHK-X addresses critical gaps by providing the first multimodal dataset specifically designed for Human Action Understanding (HAU) 
      and Human Action Reasoning (HARn).
    </p>
    <p>
      The dataset was collected from <strong>30 participants</strong> across diverse environments using our novel <strong>ActScene framework</strong> 
      - a prompt-based scene creation approach that leverages Large Language Models (LLMs) to generate logical and spatio-temporal activity descriptions. 
      This ensures both consistency and ecological validity in the collected data.
    </p>
    <p>
      CUHK-X provides three comprehensive benchmarks: <strong>HAR</strong> (Human Action Recognition), <strong>HAU</strong> (Human Action Understanding), 
      and <strong>HARn</strong> (Human Action Reasoning), encompassing eight distinct evaluation tasks. Our extensive experiments demonstrate 
      significant challenges in cross-subject and cross-domain scenarios, highlighting the dataset's value for advancing robust multimodal human activity analysis.
    </p>
  </section>

  <section id="hardware">
    <h2>Hardware Setup</h2>
    <p>
      CUHK-X was collected using a sophisticated multi-sensor setup ensuring synchronized data capture across all modalities:
    </p>
    <ul>
      <li><strong>Vzense NYX 650:</strong> RGB-D camera providing color and depth information</li>
      <li><strong>Texas Instruments Radar:</strong> mmWave sensing for privacy-preserving motion detection</li>
      <li><strong>IMU Sensors:</strong> Motion and orientation tracking with high temporal resolution</li>
      <li><strong>Thermal Cameras:</strong> Heat signature analysis for environmental robustness</li>
      <li><strong>Synchronized Recording:</strong> Temporal alignment across all modalities for consistent analysis</li>
    </ul>

    <div class="media">
      <img src="../images/hardware.png" alt="Hardware setup showing the multi-sensor configuration" class="preview">
    </div>
  </section>

  <section id="benchmarks">
    <h2>Benchmarks & Tasks</h2>
    <p>
      CUHK-X provides three comprehensive benchmarks that progressively increase in complexity, from basic recognition to advanced reasoning:
    </p>

    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 25px; margin: 30px 0;">
      <div style="border: 2px solid #007acc; padding: 20px; border-radius: 12px; background: linear-gradient(145deg, #f8feff 0%, #e6f7ff 100%);">
        <h3 style="color: #007acc; margin-top: 0;">üéØ HAR - Human Action Recognition</h3>
        <p><strong>Objective:</strong> Traditional action classification across modalities</p>
        <ul>
          <li>Cross-subject evaluation (LOSO protocol)</li>
          <li>Cross-domain performance analysis</li>
          <li>Long-tail distribution handling</li>
          <li>Multimodal fusion strategies</li>
        </ul>
      </div>

      <div style="border: 2px solid #52c41a; padding: 20px; border-radius: 12px; background: linear-gradient(145deg, #f6ffed 0%, #d9f7be 100%);">
        <h3 style="color: #52c41a; margin-top: 0;">üß† HAU - Human Action Understanding</h3>
        <p><strong>Objective:</strong> Comprehend actions through contextual integration</p>
        <ul>
          <li><strong>Action Captioning:</strong> Generate natural language descriptions</li>
          <li><strong>Emotion Analysis:</strong> Identify emotional states</li>
          <li><strong>Sequential Reordering:</strong> Organize actions chronologically</li>
          <li><strong>Action Selection:</strong> Choose relevant actions from candidates</li>
        </ul>
      </div>

      <div style="border: 2px solid #fa8c16; padding: 20px; border-radius: 12px; background: linear-gradient(145deg, #fff7e6 0%, #ffd591 100%);">
        <h3 style="color: #fa8c16; margin-top: 0;">üîÆ HARn - Human Action Reasoning</h3>
        <p><strong>Objective:</strong> Infer intentions and causal relationships</p>
        <ul>
          <li><strong>Next Action Prediction:</strong> Predict likely subsequent actions</li>
          <li><strong>Temporal Reasoning:</strong> Understand action progression logic</li>
          <li><strong>Contextual Inference:</strong> Consider environmental factors</li>
          <li><strong>Causal Understanding:</strong> Link actions to intentions</li>
        </ul>
      </div>
    </div>

    <h3>Novel ActScene Framework</h3>
    <p>
      Our innovative <strong>ActScene framework</strong> leverages Large Language Models to generate consistent, logical activity descriptions 
      that participants then perform. This approach ensures:
    </p>
    <ul>
      <li><strong>Logical Consistency:</strong> Activities follow natural progression and causality</li>
      <li><strong>Spatio-temporal Coherence:</strong> Actions are contextually appropriate</li>
      <li><strong>Human-in-the-Loop Validation:</strong> Quality assurance for generated scenarios</li>
      <li><strong>Scalable Annotation:</strong> Efficient generation of diverse scenarios</li>
    </ul>
  </section>

  <section id="dataset">
    <h2>Dataset Overview</h2>
    <p>
      CUHK-X represents a significant advancement in multimodal human activity datasets, featuring:
    </p>
    <ul>
      <li><strong>Seven Synchronized Modalities:</strong> RGB, Infrared (IR), Depth, Thermal, IMU, mmWave Radar, and Skeleton data</li>
      <li><strong>Large-Scale:</strong> 36,414 annotated action samples from 30 diverse participants</li>
      <li><strong>Dual Data Structure:</strong> Both singular actions (30,000+ samples) and sequential activities for temporal reasoning</li>
      <li><strong>Rich Annotations:</strong> LLM-generated captions with human-in-the-loop validation</li>
      <li><strong>Environmental Diversity:</strong> Indoor and outdoor settings with varying conditions</li>
    </ul>

    <h3>Modality Specifications</h3>
    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0;">
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üé• RGB Video</h4>
        <p>Standard color video recordings for traditional visual analysis</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üå°Ô∏è Infrared (IR)</h4>
        <p>Thermal imaging for robustness to lighting conditions</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üìè Depth</h4>
        <p>3D spatial information from depth cameras</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üî• Thermal</h4>
        <p>Heat signature analysis for unique behavioral cues</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üì± IMU</h4>
        <p>Inertial Measurement Unit for motion dynamics</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üì° mmWave Radar</h4>
        <p>Privacy-preserving motion detection</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>ü¶¥ Skeleton</h4>
        <p>3D pose estimation and joint tracking</p>
      </div>
    </div>

    <div class="scroll-container">
      <img src="../images/download.png" alt="Dataset overview showing modality examples">
    </div>

    <div class="scroll-container">
      <img src="../images/action.png" alt="Action categories and distribution" class="small-image">
    </div>
  </section>

<section id="data-visualization">
  <h2>Data Visualization</h2>
  <div class="video-container">
    <div class="video-block">
      <p class="caption">Depth</p>
      <video controls class="preview" preload="metadata">
        <source src="../videos/Depth_fixed.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>

    <div class="video-block">
      <p class="caption">Thermal</p>
      <video controls class="preview" preload="metadata">
        <source src="../videos/Thermal_fixed.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
  </div>

  <div class="media">
    <div class="image-block">
      <p class="caption">IMU</p>
      <img src="../images/gyro.png" alt="IMU sensor data visualization" class="preview">
    </div>
  </div>
</section>

  <section id="results">
    <h2>Experimental Results</h2>
    
    <h3>Key Findings</h3>
    <p>Our comprehensive evaluation across the three benchmarks reveals several important insights:</p>
    
    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(350px, 1fr)); gap: 20px; margin: 25px 0;">
      <div style="background: #f0f8ff; padding: 20px; border-radius: 10px; border-left: 4px solid #007acc;">
        <h4>üéØ HAR Performance (Cross-Subject LOSO)</h4>
        <table style="width: 100%; border-collapse: collapse;">
          <tr style="background: #e6f3ff;">
            <th style="padding: 8px; text-align: left;">Modality</th>
            <th style="padding: 8px; text-align: center;">Baseline</th>
            <th style="padding: 8px; text-align: center;">Best</th>
          </tr>
          <tr>
            <td style="padding: 8px; border-top: 1px solid #ddd;">RGB</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">45.2%</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;"><strong>56.56%</strong></td>
          </tr>
          <tr>
            <td style="padding: 8px; border-top: 1px solid #ddd;">IMU</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">38.7%</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">48.9%</td>
          </tr>
          <tr>
            <td style="padding: 8px; border-top: 1px solid #ddd;">Skeleton</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">41.5%</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">51.3%</td>
          </tr>
        </table>
      </div>

      <div style="background: #f6ffed; padding: 20px; border-radius: 10px; border-left: 4px solid #52c41a;">
        <h4>üß† HAU Performance Highlights</h4>
        <ul style="margin: 10px 0;">
          <li><strong>QwenVL-7B:</strong> Consistently best performer across tasks</li>
          <li><strong>VLLaVA-7B:</strong> Strong performance in depth and IR modalities</li>
          <li><strong>Emotion Analysis:</strong> Up to 77.77% accuracy with thermal imaging</li>
          <li><strong>Sequential Reordering:</strong> 68.5% accuracy for complex temporal reasoning</li>
        </ul>
      </div>

      <div style="background: #fff7e6; padding: 20px; border-radius: 10px; border-left: 4px solid #fa8c16;">
        <h4>üîÆ HARn Insights</h4>
        <ul style="margin: 10px 0;">
          <li><strong>Reasoning vs Captioning:</strong> Reasoning models significantly outperform captioning models</li>
          <li><strong>Modality Impact:</strong> Depth and IR often superior to RGB for reasoning tasks</li>
          <li><strong>Model Scale:</strong> Larger models (7B) consistently outperform smaller ones</li>
          <li><strong>Context Understanding:</strong> Critical for next action prediction accuracy</li>
        </ul>
      </div>
    </div>

    <h3>Challenging Aspects</h3>
    <div style="background: #fff2f0; padding: 20px; border-radius: 10px; border: 1px solid #ffccc7; margin: 20px 0;">
      <ul>
        <li><strong>Cross-Domain Shift:</strong> Performance drops markedly in cross-domain scenarios (similar to state-of-the-art ~60%)</li>
        <li><strong>Long-Tail Distribution:</strong> Realistic but challenging class imbalance affects rare action recognition</li>
        <li><strong>Subject Variability:</strong> Individual differences create significant challenges for generalization</li>
        <li><strong>Temporal Complexity:</strong> Sequential reasoning requires sophisticated understanding of action progression</li>
      </ul>
    </div>

    <h3>Model Performance Analysis</h3>
    <p>
      Our evaluation of state-of-the-art models reveals that <strong>larger models (7B parameters)</strong> consistently outperform 
      smaller ones across all benchmarks. <strong>QwenVL-7B</strong> and <strong>VLLaVA-7B</strong> demonstrate superior capabilities, 
      particularly in complex reasoning tasks. Interestingly, <strong>Depth and IR modalities</strong> often provide richer information 
      than RGB for understanding and reasoning tasks, highlighting the value of multimodal approaches.
    </p>
  </section>

  <section id="citation">
    <h2>Citation</h2>
    <p>If you use CUHK-X in your research, please cite our paper:</p>
    <pre>
@inproceedings{jiang2025cuhkx,
  title={CUHK-X: A Large-Scale Multimodal Dataset and Benchmark for Human Action Recognition, Understanding and Reasoning},
  author={Jiang, Siyang and Yuan, Mu and Ji, Xiang and Yang, Bufang and Liu, Zeyu and Xu, Lilin and Li, Yang and He, Yuting and Dong, Liran and Yan, Zhenyu and Jiang, Xiaofan and Gao, Wei and Chen, Hongkai and Xing, Guoliang},
  booktitle={Proceedings of the 26th International Conference on Sensing, Communication, and Networking (SenSys)},
  year={2025},
  organization={ACM}
}
    </pre>

    <!-- <h3>Related Publications</h3>
    <ul style="text-align: left; max-width: 800px; margin: 20px auto;">
      <li><strong>PGADA:</strong> <a href="https://arxiv.org/abs/2205.03817">Perturbation-Guided Adversarial Alignment for Few-Shot Learning</a> (PAKDD 2022 Best Paper)</li>
      <li><strong>ArtFL:</strong> <a href="#">Exploiting Data Resolution in Federated Learning for Dynamic Runtime Inference</a> (IPSN 2024)</li>
      <li><strong>LLM-FL:</strong> <a href="#">An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding</a> (2025)</li>
    </ul> -->

    <h3>Contact Information</h3>
    <p>For dataset access, questions, or collaborations:</p>
    <ul style="text-align: left; max-width: 600px; margin: 20px auto;">
      <li><strong>Primary Contact:</strong> <a href="mailto:siyangjiang@cuhk.edu.hk">siyangjiang@cuhk.edu.hk</a></li>
      <li><strong>Institution:</strong> The Chinese University of Hong Kong</li>
      <li><strong>Dataset Request:</strong> Please contact for access information</li>
    </ul>
  </section>

  <footer id="acknowledgments">
    <h2>Acknowledgments</h2>
    <p>
      We thank all participants who contributed to the CUHK-X dataset collection. Special acknowledgments to 
      the CUHK research team and collaborators who made this comprehensive multimodal dataset possible. 
      The hardware setup and synchronization infrastructure were crucial for achieving the quality and scale of CUHK-X.
    </p>
    <p>
      <strong>Broader Impact:</strong> CUHK-X aims to advance research in healthcare monitoring, smart environments, 
      and privacy-preserving human activity understanding. We hope this dataset serves as a valuable resource for 
      the research community to develop more robust and practical human activity recognition systems.
    </p>
    <p style="margin-top: 30px; font-size: 0.9em; color: #666;">
      This website template is adapted from <a href="https://github.com/adobe-research/custom-diffusion" target="_blank">Custom Diffusion</a>.
    </p>
  </footer>

  <script>
    // Smart smooth scroll to account for sticky navbar
    document.querySelectorAll('.nav-container a').forEach(link => {
      link.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);
        const navbarHeight = document.querySelector('.navbar').offsetHeight;
        const elementPosition = targetElement.getBoundingClientRect().top;
        const offsetPosition = elementPosition + window.pageYOffset - navbarHeight - 10;
        window.scrollTo({ top: offsetPosition, behavior: 'smooth' });
      });
    });
  </script>
</body>
</html>
