<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>CUHK-X: Multimodal Dataset for Human Action Recognition, Understanding and Reasoning</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <style>
    /* ========= General Layout ========= */
    body {
      font-family: 'Inter', sans-serif;
      color: #222;
      background: #fafafa;
      margin: 0;
      padding: 0;
      line-height: 1.6;
    }

    html {
      scroll-behavior: smooth;
    }

    main, header, section, footer {
      max-width: 950px;
      margin: 0 auto;
      padding: 40px 20px;
    }

    /* ========= Navbar ========= */
    .navbar {
      position: sticky;
      top: 0;
      width: 100%;
      background: white;
      border-bottom: 1px solid #eee;
      z-index: 999;
    }

    .nav-container {
      max-width: 950px;
      margin: 0 auto;
      display: flex;
      justify-content: center;
      padding: 12px 20px;
      gap: 24px;
    }

    .nav-container a {
      text-decoration: none;
      color: #333;
      font-weight: 500;
      transition: color 0.2s ease;
    }

    .nav-container a:hover {
      color: #007acc;
    }

    /* ========= Hero Banner ========= */
    .hero {
      text-align: center;
      padding: 100px 20px 60px;
      background: linear-gradient(180deg, #fdfdfd 0%, #fafafa 100%);
    }

    .hero h1 {
      font-size: 3em;
      font-weight: 700;
      margin-bottom: 20px;
    }

    .hero .authors {
      font-size: 1.2em;
      color: #555;
      margin-bottom: 10px;
    }

    .hero .affiliation {
      font-size: 1.1em;
      color: #666;
      margin-bottom: 30px;
      font-style: italic;
    }

    .hero .links a {
      display: inline-block;
      margin: 0 12px;
      padding: 10px 18px;
      border: 1px solid #333;
      border-radius: 10px;
      text-decoration: none;
      color: #333;
      font-weight: 500;
      transition: all 0.2s ease;
    }

    .hero .links a:hover {
      background: #333;
      color: white;
    }

    /* ========= Section Titles ========= */
    section h2 {
      text-align: center;
      font-size: 2em;
      margin-bottom: 20px;
      font-weight: 600;
    }

    /* ========= Media (images, videos) ========= */
    .media {
      display: flex;
      justify-content: center;
      margin: 40px 0;
    }

    .preview {
      max-width: 90%;
      border: 1px solid #ddd;
      border-radius: 12px;
      box-shadow: 0 4px 16px rgba(0,0,0,0.08);
      transition: transform 0.2s ease;
    }

    .preview:hover {
      transform: scale(1.02);
    }

    .video-container {
      display: flex;
      justify-content: center;
      align-items: center;
      gap: 20px;
      flex-wrap: wrap;
      margin-top: 30px;
    }

    .video-container .preview {
      width: 440px;   /* equal width for both videos */
      height: auto;
      border: 1px solid #ddd;
      border-radius: 12px;
      box-shadow: 0 4px 16px rgba(0,0,0,0.08);
      max-width: 100%; /* ensure responsiveness */
    }

    /* Video specific styling */
    video {
      display: block;
      width: 100%;
      height: auto;
    }

    /* Responsive video container */
    @media (max-width: 768px) {
      .video-container {
        flex-direction: column;
      }
      
      .video-container .preview {
        width: 100%;
        max-width: 400px;
      }
    }

    .video-block {
      text-align: center; /* centers caption and video */
    }

    .caption {
      font-weight: bold;
      text-align: center;
      margin-bottom: 8px;
      font-size: 1rem;
      color: #333;
    }

    .image-block {
      text-align: center;
      margin-top: 30px;
    }

    .image-block .caption {
      font-weight: bold;
      margin-bottom: 8px;
      font-size: 1rem;
      color: #333;
    }

    /* ========= Scrollable Image ========= */
    .scroll-container {
      max-width: 100%;
      height: 600px;
      overflow-y: auto;
      overflow-x: hidden;
      border: 1px solid #ddd;
      border-radius: 8px;
      margin: 30px auto;
    }

    .scroll-container img {
      display: block;
      width: 100%;
      height: auto;
    }

    .scroll-container img.small-image {
      width: 60%;         /* try 20% or even 10% */
      max-width: none;    /* disables any previous max-width rule */
      height: auto;
      display: block;
      margin: 0 auto;
    }

    /* ========= Code block (Citation) ========= */
    pre {
      background: #f4f4f4;
      padding: 16px;
      border-radius: 8px;
      overflow-x: auto;
      font-size: 0.9em;
      max-width: 800px;
      margin: 0 auto;
    }

    /* ========= Footer / Acknowledgments ========= */
    footer {
      text-align: center;
      padding: 60px 20px;
      background: #f9f9f9;
      border-top: 1px solid #eee;
      margin-top: 80px;
    }

    footer h2 {
      font-size: 1.8em;
      margin-bottom: 20px;
    }

    footer p {
      font-size: 1em;
      color: #444;
      max-width: 800px;
      margin: 0 auto;
      line-height: 1.6;
    }

    footer a {
      color: #0077cc;
      text-decoration: none;
      font-weight: 500;
    }

    footer a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>
  <nav class="navbar">
    <div class="nav-container">
      <a href="#abstract">Abstract</a>
      <a href="#highlights">Highlights</a>
      <a href="#hardware">Hardware</a>
      <a href="#environment-setup">Environment</a>
      <a href="#dataset">Dataset</a>
      <a href="#data-visualization">Visualization</a>
      <a href="#benchmarks">Benchmarks</a>
      <a href="#results">Results</a>
      <!-- <a href="#citation">Citation</a> -->
    </div>
  </nav>

  <header class="hero">
    <h1>CUHK-X</h1>
    <h2>A Large-Scale Multimodal Dataset and Benchmark for Human Action Recognition, Understanding and Reasoning</h2>
    <p class="authors">
    Siyang Jiang, Mu Yuan, Xiang Ji, Bufang Yang, Zeyu Liu, Lilin Xu, Yang Li, Yuting He, Liran Dong, Wenrui Lu,
    Zhenyu Yan, Xiaofan Jiang, Wei Gao, Hongkai Chen, Guoliang Xing.
    </p>
    <p class="affiliation">The Chinese University of Hong Kong (CUHK), University of Illinois Urbana-Champaign (UIUC), University of Pittsburgh (Pitt), Columbia University (Columbia)</p>

    <div class="links">
      <a href="../paper/cuhkx.pdf">[Paper]</a>
      <a href="#">[Dataset]</a>
      <a href="https://github.com/siyang-jiang/CUHK-X">[Code]</a>
      <a href="#data-visualization">[Demo]</a>
    </div>
  </header>

  <section id="abstract">
    <h2>Abstract</h2>
    <p>
      CUHK-X is a comprehensive multimodal dataset containing <strong>58,445 samples</strong> across <strong>seven modalities</strong> 
      designed for human activity recognition, understanding, and reasoning. Unlike existing datasets that focus primarily on recognition tasks, 
      CUHK-X addresses critical gaps by providing the first multimodal dataset specifically designed for Human Action Understanding (HAU) 
      and Human Action Reasoning (HARn).
    </p>
    <p>
      The dataset was collected from <strong>30 participants</strong> across diverse environments using our novel <strong> framework</strong> 
      - a prompt-based scene creation approach that leverages Large Language Models (LLMs) to generate logical and spatio-temporal activity descriptions. 
      This ensures both consistency and ecological validity in the collected data.
    </p>
    <p>
      CUHK-X provides three comprehensive benchmarks: <strong>HAR</strong> (Human Action Recognition), <strong>HAU</strong> (Human Action Understanding), 
      and <strong>HARn</strong> (Human Action Reasoning), encompassing eight distinct evaluation tasks. Our extensive experiments demonstrate 
      significant challenges in cross-subject and cross-domain scenarios, highlighting the dataset's value for advancing robust multimodal human activity analysis.
    </p>
  </section>

  <section id="highlights">
    <h2>Highlights</h2>
    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 25px; margin: 30px 0;">
      <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 25px; border-radius: 15px; box-shadow: 0 8px 32px rgba(0,0,0,0.1);">
        <h3 style="color: white; margin-top: 0; font-size: 1.4em;">üéØ First of its Kind</h3>
        <p style="margin-bottom: 0;">First multimodal dataset specifically designed for Human Action Understanding (HAU) and Human Action Reasoning (HARn) beyond traditional recognition tasks.</p>
      </div>
      
      <div style="background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; padding: 25px; border-radius: 15px; box-shadow: 0 8px 32px rgba(0,0,0,0.1);">
        <h3 style="color: white; margin-top: 0; font-size: 1.4em;">üìä Large-Scale & Diverse</h3>
        <p style="margin-bottom: 0;">58,445 annotated samples across 7 synchronized modalities from 30 diverse participants, covering both indoor and outdoor environments.</p>
      </div>
      
      <div style="background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%); color: white; padding: 25px; border-radius: 15px; box-shadow: 0 8px 32px rgba(0,0,0,0.1);">
        <h3 style="color: white; margin-top: 0; font-size: 1.4em;">ü§ñ Framework</h3>
        <p style="margin-bottom: 0;">Novel LLM-powered framework for generating logical, consistent activity scenarios with human-in-the-loop validation.</p>
      </div>
      
      <div style="background: linear-gradient(135deg, #fa709a 0%, #fee140 100%); color: white; padding: 25px; border-radius: 15px; box-shadow: 0 8px 32px rgba(0,0,0,0.1);">
        <h3 style="color: white; margin-top: 0; font-size: 1.4em;">üî¨ Three Benchmarks</h3>
        <p style="margin-bottom: 0;">Comprehensive evaluation across HAR, HAU, and HARn with 8 distinct tasks, from basic recognition to complex reasoning.</p>
      </div>
      
      <div style="background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); color: #333; padding: 25px; border-radius: 15px; box-shadow: 0 8px 32px rgba(0,0,0,0.1);">
        <h3 style="color: #333; margin-top: 0; font-size: 1.4em;">üåç Real-World Impact</h3>
        <p style="margin-bottom: 0;">Designed for healthcare monitoring, smart environments, and privacy-preserving human activity understanding applications.</p>
      </div>
      
      <div style="background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%); color: #333; padding: 25px; border-radius: 15px; box-shadow: 0 8px 32px rgba(0,0,0,0.1);">
        <h3 style="color: #333; margin-top: 0; font-size: 1.4em;">üìà Challenging Results</h3>
        <p style="margin-bottom: 0;">Reveals significant challenges in cross-subject and cross-domain scenarios, pushing the boundaries of current SOTA models.</p>
      </div>
    </div>
    
    <div style="background: #f8f9fa; padding: 25px; border-radius: 12px; margin: 30px 0; border-left: 5px solid #007acc;">
      <h3 style="color: #007acc; margin-top: 0;">üèÜ Key Achievements</h3>
      <ul style="list-style: none; padding: 0; margin: 0;">
        <li style="padding: 8px 0; display: flex; align-items: center;"><span style="color: #52c41a; margin-right: 10px; font-size: 1.2em;">‚úì</span> Synchronized multi-sensor data with precise temporal alignment</li>
        <li style="padding: 8px 0; display: flex; align-items: center;"><span style="color: #52c41a; margin-right: 10px; font-size: 1.2em;">‚úì</span> Comprehensive evaluation of state-of-the-art multimodal models</li>
        <li style="padding: 8px 0; display: flex; align-items: center;"><span style="color: #52c41a; margin-right: 10px; font-size: 1.2em;">‚úì</span> Open-source framework and benchmarking tools</li>
      </ul>
    </div>
  </section>

  <section id="hardware">
    <h2>Data Acquisition Infrastructure</h2>
    <p>
      The CUHK-X dataset was systematically collected using a comprehensive multi-sensor platform designed to capture 
      complementary modalities with precise temporal synchronization. Our instrumentation framework consists of seven 
      distinct sensing modalities, each contributing unique perspectives for robust human activity analysis.
    </p>

    <h3>Sensor Specifications and Technical Details</h3>
    
    <div style="display: grid; grid-template-columns: 1fr; gap: 20px; margin: 25px 0;">
      
      <div style="background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px;">
        <h4 style="color: #0366d6; margin-top: 0; display: flex; align-items: center;">
          <span style="margin-right: 10px;">üì∑</span> RGB-D Sensing Module
        </h4>
        <p><strong>Device:</strong> Vzense NYX 650 Time-of-Flight Camera</p>
        <ul style="margin: 10px 0; padding-left: 20px;">
          <li><strong>RGB Resolution:</strong> 1920√ó1080 @ 30 FPS</li>
          <li><strong>Depth Resolution:</strong> 640√ó480 @ 30 FPS</li>
          <li><strong>Depth Range:</strong> 0.15m - 5.0m with ¬±1% accuracy</li>
          <li><strong>Field of View:</strong> 69¬∞ (H) √ó 42¬∞ (V) for RGB, 70¬∞ (H) √ó 55¬∞ (V) for depth</li>
          <li><strong>Interface:</strong> USB 3.0 with hardware synchronization capabilities</li>
        </ul>
      </div>

      <div style="background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px;">
        <h4 style="color: #0366d6; margin-top: 0; display: flex; align-items: center;">
          <span style="margin-right: 10px;">üì°</span> mmWave Radar System
        </h4>
        <p><strong>Device:</strong> Texas Instruments IWR6843ISK mmWave Sensor</p>
        <ul style="margin: 10px 0; padding-left: 20px;">
          <li><strong>Frequency Band:</strong> 60-64 GHz with 4 GHz bandwidth</li>
          <li><strong>Range Resolution:</strong> 3.75 cm with maximum range of 6m</li>
          <li><strong>Velocity Resolution:</strong> 0.13 m/s with ¬±9.16 m/s range</li>
          <li><strong>Angular Resolution:</strong> 15¬∞ azimuth, 60¬∞ elevation</li>
          <li><strong>Frame Rate:</strong> 20 FPS for real-time motion tracking</li>
        </ul>
      </div>

      <div style="background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px;">
        <h4 style="color: #0366d6; margin-top: 0; display: flex; align-items: center;">
          <span style="margin-right: 10px;">üå°Ô∏è</span> Thermal Imaging Array
        </h4>
        <p><strong>Device:</strong> FLIR Lepton 3.5 Microbolometer</p>
        <ul style="margin: 10px 0; padding-left: 20px;">
          <li><strong>Spectral Range:</strong> 8-14 Œºm longwave infrared</li>
          <li><strong>Resolution:</strong> 160√ó120 pixels @ 9 Hz</li>
          <li><strong>Temperature Range:</strong> -10¬∞C to +80¬∞C scene temperature</li>
          <li><strong>Thermal Sensitivity:</strong> <50 mK NETD typical</li>
          <li><strong>Field of View:</strong> 57¬∞ √ó 44¬∞ with fixed focus</li>
        </ul>
      </div>

      <div style="background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 8px; padding: 20px;">
        <h4 style="color: #0366d6; margin-top: 0; display: flex; align-items: center;">
          <span style="margin-right: 10px;">üì±</span> Inertial Measurement Unit
        </h4>
        <p><strong>Device:</strong> MPU-9250 Nine-Axis Motion Tracking</p>
        <ul style="margin: 10px 0; padding-left: 20px;">
          <li><strong>Accelerometer:</strong> ¬±2/¬±4/¬±8/¬±16g range, 16-bit ADC</li>
          <li><strong>Gyroscope:</strong> ¬±250/¬±500/¬±1000/¬±2000 dps, 16-bit ADC</li>
          <li><strong>Magnetometer:</strong> ¬±4800 ŒºT range with 14-bit ADC</li>
          <li><strong>Sampling Rate:</strong> Up to 1 kHz for accelerometer and gyroscope</li>
          <li><strong>Interface:</strong> I2C/SPI with interrupt-driven data ready</li>
        </ul>
      </div>

    </div>

    <h3>Synchronization and Calibration Protocol</h3>
    <div style="background: #e7f3ff; border-left: 4px solid #007acc; padding: 20px; margin: 20px 0; border-radius: 0 8px 8px 0;">
      <p>
        <strong>Temporal Synchronization:</strong> All sensors are synchronized using a master clock signal generated by the 
        host computer, ensuring sub-millisecond accuracy across modalities. Hardware timestamps are recorded for each frame 
        to enable precise temporal alignment during post-processing.
      </p>
      <p>
        <strong>Spatial Calibration:</strong> Intrinsic and extrinsic camera parameters are determined using Zhang's method 
        with a checkerboard calibration target. Cross-modal calibration matrices are computed to register depth, thermal, 
        and RGB coordinate systems to a common reference frame.
      </p>
      <p>
        <strong>Data Validation:</strong> Each recording session includes calibration verification sequences to ensure 
        measurement accuracy and detect potential sensor drift or misalignment.
      </p>
    </div>

    <h3>Hardware Systems</h3>
      <p>
        The data acquisition system operates on a distributed architecture where individual sensor modules communicate 
        with a central processing unit via high-speed interfaces (USB 3.0, Ethernet). Real-time data streaming and 
        buffering capabilities ensure no frame drops during extended recording sessions. The system supports both 
        triggered and continuous acquisition modes, with automatic quality assessment and error recovery mechanisms.
      </p>

    <div class="media">
      <img src="https://github.com/siyang-jiang/CUHK-X/blob/main/images/hardware.png?raw=true" alt="Multi-sensor data acquisition platform showing synchronized sensor array and processing infrastructure" class="preview">
    </div>
  </section>

  <section id="environment-setup">
    <h2>Environment Setup</h2>
    <p>
      To reproduce our results and work with the CUHK-X dataset, follow these comprehensive setup instructions:
    </p>

    <div class="media">
      <img src="../images/env.png" alt="Environment setup and dependencies overview" class="preview">
    </div>


  <section id="dataset">
    <h2>Dataset Overview</h2>
    <p>
      CUHK-X represents a significant advancement in multimodal human activity datasets, featuring:
    </p>
    <ul>
      <li><strong>Seven Synchronized Modalities:</strong> RGB, Infrared (IR), Depth, Thermal, IMU, mmWave Radar, and Skeleton data</li>
      <li><strong>Large-Scale:</strong> 58,445 annotated action samples from 30 diverse participants</li>
      <li><strong>Dual Data Structure:</strong> Both singular actions and sequential activities for temporal reasoning</li>
      <li><strong>Rich Annotations:</strong> LLM-generated captions with human-in-the-loop validation</li>
      <li><strong>Environmental Diversity:</strong> Indoor and outdoor settings with varying conditions</li>
    </ul>

    <div class="scroll-container">
      <img src="../images/action.png" alt="Action categories and distribution" class="small-image">
    </div>

    <h3>Modality Specifications</h3>
    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 0px; margin: 0 0;">
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üé• RGB Video</h4>
        <p>Standard color video recordings for traditional visual analysis</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üå°Ô∏è Infrared (IR)</h4>
        <p>Thermal imaging for robustness to lighting conditions</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üìè Depth</h4>
        <p>3D spatial information from depth cameras</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üî• Thermal</h4>
        <p>Heat signature analysis for unique behavioral cues</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üì± IMU</h4>
        <p>Inertial Measurement Unit for motion dynamics</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>üì° mmWave Radar</h4>
        <p>Privacy-preserving motion detection</p>
      </div>
      <div style="background: #f8f9fa; padding: 15px; border-radius: 8px;">
        <h4>ü¶¥ Skeleton</h4>
        <p>3D pose estimation and joint tracking</p>
      </div>
    </div>


  </section>

<section id="data-visualization">
  <h2>Data Visualization</h2>
    <div class="scroll-container">
      <img src="../images/download.png" alt="Dataset overview showing modality examples">
    </div>


  <div class="video-container">
    <div class="video-block">
      <p class="caption">Depth</p>
      <video controls class="preview" preload="metadata">
        <source src="../videos/Depth_fixed.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>

    <div class="video-block">
      <p class="caption">Thermal</p>
      <video controls class="preview" preload="metadata">
        <source src="../videos/Thermal_fixed.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
  </div>

  <div class="media">
    <div class="image-block">
      <p class="caption">IMU</p>
      <img src="../images/gyro.png" alt="IMU sensor data visualization" class="preview">
    </div>
  </div>
</section>

<section id="benchmarks">
    <h2>Benchmarks & Tasks</h2>
    <p>
      CUHK-X provides three comprehensive benchmarks that progressively increase in complexity, from basic recognition to advanced reasoning:
    </p>

    <div style="display: flex; gap: 20px; margin: 30px 0; flex-wrap: wrap;">
      <div style="flex: 1; min-width: 280px; border: 2px solid #007acc; padding: 20px; border-radius: 12px; background: linear-gradient(145deg, #f8feff 0%, #e6f7ff 100%);">
        <h3 style="color: #007acc; margin-top: 0;">üéØ HAR - Human Action Recognition</h3>
        <p><strong>Objective:</strong> Traditional action classification across modalities</p>
        <ul>
          <li>Cross-subject evaluation (LOSO protocol)</li>
          <li>Cross-domain performance analysis</li>
          <li>Long-tail distribution handling</li>
          <li>Multimodal fusion strategies</li>
        </ul>
      </div>

      <div style="flex: 1; min-width: 280px; border: 2px solid #52c41a; padding: 20px; border-radius: 12px; background: linear-gradient(145deg, #f6ffed 0%, #d9f7be 100%);">
        <h3 style="color: #52c41a; margin-top: 0;">üß† HAU - Human Action Understanding</h3>
        <p><strong>Objective:</strong> Comprehend actions through contextual integration</p>
        <ul>
          <li><strong>Action Captioning:</strong> Generate natural language descriptions</li>
          <li><strong>Emotion Analysis:</strong> Identify emotional states</li>
          <li><strong>Sequential Reordering:</strong> Organize actions chronologically</li>
          <li><strong>Action Selection:</strong> Choose relevant actions from candidates</li>
        </ul>
      </div>

      <div style="flex: 1; min-width: 280px; border: 2px solid #fa8c16; padding: 20px; border-radius: 12px; background: linear-gradient(145deg, #fff7e6 0%, #ffd591 100%);">
        <h3 style="color: #fa8c16; margin-top: 0;">üîÆ HARn - Human Action Reasoning</h3>
        <p><strong>Objective:</strong> Infer intentions and causal relationships</p>
        <ul>
          <li><strong>Next Action Prediction:</strong> Predict likely subsequent actions</li>
          <li><strong>Temporal Reasoning:</strong> Understand action progression logic</li>
          <li><strong>Contextual Inference:</strong> Consider environmental factors</li>
          <li><strong>Causal Understanding:</strong> Link actions to intentions</li>
        </ul>
      </div>
    </div>

    <h3>Novel Framework</h3>
    <p>
      Our innovative <strong> framework</strong> leverages Large Language Models to generate consistent, logical activity descriptions 
      that participants then perform. This approach ensures:
    </p>
    <ul>
      <li><strong>Logical Consistency:</strong> Activities follow natural progression and causality</li>
      <li><strong>Spatio-temporal Coherence:</strong> Actions are contextually appropriate</li>
      <li><strong>Human-in-the-Loop Validation:</strong> Quality assurance for generated scenarios</li>
      <li><strong>Scalable Annotation:</strong> Efficient generation of diverse scenarios</li>
    </ul>
  </section>

  <section id="results">
    <h2>Experimental Results</h2>
    
    <h3>Key Findings</h3>
    <p>Our comprehensive evaluation across the three benchmarks reveals several important insights from extensive experiments on state-of-the-art models:</p>
    
    <div style="display: flex; gap: 20px; margin: 25px 0; flex-wrap: wrap;">
      <div style="flex: 1; min-width: 320px; background: #f0f8ff; padding: 20px; border-radius: 10px; border-left: 4px solid #007acc;">
        <h4>üéØ HAR Performance (Cross-Subject LOSO)</h4>
        <table style="width: 100%; border-collapse: collapse;">
          <tr style="background: #e6f3ff;">
            <th style="padding: 8px; text-align: left;">Modality</th>
            <th style="padding: 8px; text-align: center;">Baseline</th>
            <th style="padding: 8px; text-align: center;">w/ Contrastive</th>
            <th style="padding: 8px; text-align: center;">w/o Cross-domain</th>
          </tr>
          <tr>
            <td style="padding: 8px; border-top: 1px solid #ddd;">RGB</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">45.2%</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">52.8%</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;"><strong>56.56%</strong></td>
          </tr>
          <tr>
            <td style="padding: 8px; border-top: 1px solid #ddd;">IMU</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">38.7%</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">44.3%</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">48.9%</td>
          </tr>
          <tr>
            <td style="padding: 8px; border-top: 1px solid #ddd;">Skeleton</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">41.5%</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">47.2%</td>
            <td style="padding: 8px; text-align: center; border-top: 1px solid #ddd;">51.3%</td>
          </tr>
        </table>
      </div>

      <div style="flex: 1; min-width: 320px; background: #f6ffed; padding: 20px; border-radius: 10px; border-left: 4px solid #52c41a;">
        <h4>üß† HAU Performance (Selected Tasks)</h4>
        <table style="width: 100%; border-collapse: collapse; font-size: 0.9em;">
          <tr style="background: #e6f7ff;">
            <th style="padding: 6px; text-align: left;">Model</th>
            <th style="padding: 6px; text-align: center;">Captioning (BLEU-1)</th>
            <th style="padding: 6px; text-align: center;">Emotion Analysis</th>
            <th style="padding: 6px; text-align: center;">Sequential Reordering</th>
          </tr>
          <tr>
            <td style="padding: 6px; border-top: 1px solid #ddd;"><strong>QwenVL-7B</strong></td>
            <td style="padding: 6px; text-align: center; border-top: 1px solid #ddd;"><strong>55.97%</strong></td>
            <td style="padding: 6px; text-align: center; border-top: 1px solid #ddd;"><strong>77.77%</strong></td>
            <td style="padding: 6px; text-align: center; border-top: 1px solid #ddd;"><strong>68.5%</strong></td>
          </tr>
          <tr>
            <td style="padding: 6px; border-top: 1px solid #ddd;">VLLaVA-7B</td>
            <td style="padding: 6px; text-align: center; border-top: 1px solid #ddd;">22.32%</td>
            <td style="padding: 6px; text-align: center; border-top: 1px solid #ddd;">74.2%</td>
            <td style="padding: 6px; text-align: center; border-top: 1px solid #ddd;">66.8%</td>
          </tr>
          <tr>
            <td style="padding: 6px; border-top: 1px solid #ddd;">InternVL-8B</td>
            <td style="padding: 6px; text-align: center; border-top: 1px solid #ddd;">0.59%</td>
            <td style="padding: 6px; text-align: center; border-top: 1px solid #ddd;">35.35%</td>
            <td style="padding: 6px; text-align: center; border-top: 1px solid #ddd;">45.3%</td>
          </tr>
        </table>
      </div>

      <div style="flex: 1; min-width: 320px; background: #fff7e6; padding: 20px; border-radius: 10px; border-left: 4px solid #fa8c16;">
        <h4>üîÆ HARn Key Insights</h4>
        <ul style="margin: 10px 0;">
          <li><strong>Model Scale Effect:</strong> Larger models (7B+) consistently outperform smaller variants</li>
          <li><strong>Modality Advantage:</strong> Depth and IR often superior to RGB for reasoning tasks</li>
          <li><strong>Reasoning vs Captioning:</strong> Specialized reasoning models show significant advantages</li>
          <li><strong>Context Dependency:</strong> Temporal context crucial for next action prediction accuracy</li>
        </ul>
      </div>
    </div>

    <h3>Dataset Organization & Tasks</h3>
    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 25px 0;">
      <div style="background: #e8f4f8; padding: 20px; border-radius: 10px; border-left: 4px solid #1890ff;">
        <h4>üìä Small Model Data</h4>
        <ul style="margin: 10px 0;">
          <li><strong>Focus:</strong> Singular, well-defined actions</li>
          <li><strong>Actions:</strong> 40+ different action categories</li>
          <li><strong>Samples:</strong> 30,000+ individual action instances</li>
          <li><strong>Purpose:</strong> Traditional HAR evaluation and baseline comparison</li>
        </ul>
      </div>

      <div style="background: #f4f0ff; padding: 20px; border-radius: 10px; border-left: 4px solid #722ed1;">
        <h4>üß† Large Model Data</h4>
        <ul style="margin: 10px 0;">
          <li><strong>Focus:</strong> Sequential actions performed consecutively</li>
          <li><strong>Features:</strong> Multi-step activity sequences with logical flow</li>
          <li><strong>Purpose:</strong> Temporal and emotional analysis, complex reasoning</li>
          <li><strong>Applications:</strong> HAU and HARn evaluation tasks</li>
        </ul>
      </div>
    </div>



    <h3>Challenging Aspects & Research Opportunities</h3>
    <div style="background: #fff2f0; padding: 20px; border-radius: 10px; border: 1px solid #ffccc7; margin: 20px 0;">
      <h4 style="color: #d4380d; margin-top: 0;">Key Research Challenges</h4>
      <ul>
        <li><strong>Cross-Domain Generalization:</strong> Performance drops significantly in cross-domain scenarios, highlighting the domain adaptation challenges in real-world deployment</li>
        <li><strong>Long-Tail Distribution:</strong> Realistic but challenging class imbalance with rare actions requiring specialized handling techniques</li>
        <li><strong>Subject Variability:</strong> Individual behavioral differences create substantial challenges for person-independent recognition systems</li>
        <li><strong>Temporal Complexity:</strong> Sequential reasoning demands sophisticated understanding of action causality and temporal dependencies</li>
        <li><strong>Multimodal Integration:</strong> Optimal fusion strategies vary significantly across tasks and environmental conditions</li>
      </ul>
    </div>

    <h3>Technical Insights & Analysis</h3>
    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 25px 0;">
      <div style="background: #f0f9ff; padding: 20px; border-radius: 10px; border-left: 4px solid #0ea5e9;">
        <h4>üîç Model Scale Effects</h4>
        <p>Systematic evaluation reveals that <strong>larger models (7B+ parameters)</strong> consistently outperform smaller variants across all three benchmarks. This trend is particularly pronounced in reasoning tasks where model capacity directly impacts performance.</p>
      </div>

      <div style="background: #ecfdf5; padding: 20px; border-radius: 10px; border-left: 4px solid #10b981;">
        <h4>üìä Modality Contributions</h4>
        <p><strong>Depth and IR modalities</strong> often provide richer contextual information than RGB for understanding and reasoning tasks, challenging conventional RGB-centric approaches and highlighting multimodal advantages.</p>
      </div>

      <div style="background: #fefce8; padding: 20px; border-radius: 10px; border-left: 4px solid #eab308;">
        <h4>üéØ Task-Specific Findings</h4>
        <ul style="margin: 10px 0;">
          <li><strong>Action Captioning:</strong> QwenVL-7B achieves 55.97% BLEU-1 score</li>
          <li><strong>Emotion Analysis:</strong> Thermal imaging enables 77.77% accuracy</li>
          <li><strong>Sequential Reasoning:</strong> 68.5% accuracy demonstrates temporal understanding capability</li>
        </ul>
      </div>
    </div>

    <h3>Broader Impact & Applications</h3>
    <div style="background: #f8fafc; padding: 20px; border-radius: 10px; border: 1px solid #e2e8f0; margin: 20px 0;">
      <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px;">
        <div>
          <h4 style="color: #1e40af;">üè• Healthcare & Monitoring</h4>
          <ul style="font-size: 0.9em;">
            <li>Cognitive decline detection through behavioral patterns</li>
            <li>Activities of daily living (ADL) assessment</li>
            <li>Rehabilitation progress tracking</li>
          </ul>
        </div>
        <div>
          <h4 style="color: #059669;">üè† Smart Environments</h4>
          <ul style="font-size: 0.9em;">
            <li>Context-aware home automation systems</li>
            <li>Security and anomaly detection</li>
            <li>Natural human-computer interaction</li>
          </ul>
        </div>
        <div>
          <h4 style="color: #7c3aed;">üî¨ Research & Education</h4>
          <ul style="font-size: 0.9em;">
            <li>Multimodal learning algorithm development</li>
            <li>Privacy-preserving AI research</li>
            <li>Standard benchmark for temporal reasoning</li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <section id="citation">

    <h2>Contact Information</h2>
    <p>For dataset access, questions, or collaborations:</p>
    <ul style="text-align: left; max-width: 600px; margin: 20px auto;">
      <li><strong>Primary Contact:</strong> <a href="mailto:siyangjiang@cuhk.edu.hk">syjiang [AT] ie.cuhk.edu.hk</a></li>
      <li><strong>Institution:</strong> The Chinese University of Hong Kong</li>
      <li><strong>Dataset Request:</strong> Please contact for access information</li>
      <li><strong>Acknowledgments:</strong>  We thank all participants who contributed to the CUHK-X dataset collection. Special acknowledgments to 
      the CUHK research team and collaborators who made this comprehensive multimodal dataset possible. 
      The hardware setup and synchronization infrastructure were crucial for achieving the quality and scale of CUHK-X.
    </li>

    </ul>
  </section>

  <script>
    // Smart smooth scroll to account for sticky navbar
    document.querySelectorAll('.nav-container a').forEach(link => {
      link.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);
        const navbarHeight = document.querySelector('.navbar').offsetHeight;
        const elementPosition = targetElement.getBoundingClientRect().top;
        const offsetPosition = elementPosition + window.pageYOffset - navbarHeight - 10;
        window.scrollTo({ top: offsetPosition, behavior: 'smooth' });
      });
    });
  </script>
</body>
</html>
